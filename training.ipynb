{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishvinodkumar/GPT2/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install tiktoken\n",
        "# !pip3 install torch\n",
        "# !pip3 install transformers"
      ],
      "metadata": {
        "id": "oZZM6xmkPnkj"
      },
      "id": "oZZM6xmkPnkj",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "64e57c99",
      "metadata": {
        "id": "64e57c99"
      },
      "source": [
        "# Training Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fbbcacd",
      "metadata": {
        "id": "0fbbcacd"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fb214508",
      "metadata": {
        "id": "fb214508"
      },
      "outputs": [],
      "source": [
        "from gpt2 import GPT2, GPT2Config, DataLoaderLite\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3483022",
      "metadata": {
        "id": "e3483022"
      },
      "source": [
        "### Set Device & Args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "85e5733b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85e5733b",
        "outputId": "373125f9-6e03-472d-c744-ae6be87225ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "print(f'Using Device: {device}')\n",
        "\n",
        "num_return_sequences = 5\n",
        "max_length = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3c47d2dd",
      "metadata": {
        "id": "3c47d2dd"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "503b104c",
      "metadata": {
        "id": "503b104c"
      },
      "source": [
        "### Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "278d1209",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "278d1209",
        "outputId": "949f8def-01f3-4300-ea05-78b469a635aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Desired Batch Size: 524288\n",
            "Grand Accumulate Steps: 32\n",
            "Loaded 338025 tokens\n",
            "1 Epoch = 20 batches\n"
          ]
        }
      ],
      "source": [
        "total_batch_size = 524288 # 2**19, ~0.5M, as per GPT paper.\n",
        "B = 16 # Micro Batch Size\n",
        "T = 1024 # Max Sequence Length\n",
        "assert total_batch_size % (B*T) == 0, \"Total Batch Size must be divisible by B*T\"\n",
        "grand_accum_steps = total_batch_size // (B*T)\n",
        "\n",
        "print(f'Total Desired Batch Size: {total_batch_size}')\n",
        "print(f'Grand Accumulate Steps: {grand_accum_steps}')\n",
        "\n",
        "input_text = './data/input.txt'\n",
        "train_loader = DataLoaderLite(B=B, T=T, input_text=input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f9bd40",
      "metadata": {
        "id": "42f9bd40"
      },
      "source": [
        "### Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set precision to TF32 when available. Will speed up total performance.\n",
        "# TF32 will reduce the decimal precision.\n",
        "torch.set_float32_matmul_precision('high')"
      ],
      "metadata": {
        "id": "XAOFOMOWLtYn"
      },
      "id": "XAOFOMOWLtYn",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2e75a347",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e75a347",
        "outputId": "9a1c7b5a-3fe2-4b0b-b63c-d4abf0316c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step: 0 | loss: 1.0939e+01 | lr: 6.0000e-05 | norm: 27.0126 | dt: 35394.091 | tokens/sec: 14812.868\n",
            "step: 1 | loss: 9.6493e+00 | lr: 1.2000e-04 | norm: 9.5178 | dt: 2783.526 | tokens/sec: 188353.879\n",
            "step: 2 | loss: 9.2256e+00 | lr: 1.8000e-04 | norm: 5.7292 | dt: 2781.923 | tokens/sec: 188462.421\n",
            "step: 3 | loss: 9.8131e+00 | lr: 2.4000e-04 | norm: 8.2066 | dt: 2781.04 | tokens/sec: 188522.266\n",
            "step: 4 | loss: 9.1916e+00 | lr: 3.0000e-04 | norm: 4.2994 | dt: 2781.193 | tokens/sec: 188511.891\n",
            "step: 5 | loss: 8.6780e+00 | lr: 3.6000e-04 | norm: 3.6285 | dt: 2783.095 | tokens/sec: 188383.069\n",
            "step: 6 | loss: 8.2950e+00 | lr: 4.2000e-04 | norm: 1.9535 | dt: 2781.343 | tokens/sec: 188501.759\n",
            "step: 7 | loss: 8.0680e+00 | lr: 4.8000e-04 | norm: 2.8519 | dt: 2781.697 | tokens/sec: 188477.783\n",
            "step: 8 | loss: 7.7142e+00 | lr: 5.4000e-04 | norm: 1.9108 | dt: 2781.67 | tokens/sec: 188479.592\n",
            "step: 9 | loss: 7.3471e+00 | lr: 6.0000e-04 | norm: 1.8006 | dt: 2782.888 | tokens/sec: 188397.062\n",
            "step: 10 | loss: 7.0297e+00 | lr: 6.0000e-04 | norm: 1.8391 | dt: 2785.74 | tokens/sec: 188204.218\n",
            "step: 11 | loss: 6.7412e+00 | lr: 5.9917e-04 | norm: 1.5059 | dt: 2787.548 | tokens/sec: 188082.138\n",
            "step: 12 | loss: 6.5289e+00 | lr: 5.9668e-04 | norm: 1.1515 | dt: 2787.789 | tokens/sec: 188065.86\n",
            "step: 13 | loss: 6.3773e+00 | lr: 5.9254e-04 | norm: 1.0573 | dt: 2787.503 | tokens/sec: 188085.162\n",
            "step: 14 | loss: 6.3416e+00 | lr: 5.8679e-04 | norm: 2.6022 | dt: 2785.701 | tokens/sec: 188206.876\n",
            "step: 15 | loss: 6.2435e+00 | lr: 5.7945e-04 | norm: 0.9697 | dt: 2790.241 | tokens/sec: 187900.599\n",
            "step: 16 | loss: 6.2125e+00 | lr: 5.7057e-04 | norm: 0.7735 | dt: 2793.661 | tokens/sec: 187670.611\n",
            "step: 17 | loss: 6.2100e+00 | lr: 5.6021e-04 | norm: 1.1343 | dt: 2793.986 | tokens/sec: 187648.784\n",
            "step: 18 | loss: 6.1568e+00 | lr: 5.4843e-04 | norm: 1.0179 | dt: 2796.029 | tokens/sec: 187511.624\n",
            "step: 19 | loss: 6.1473e+00 | lr: 5.3531e-04 | norm: 1.4984 | dt: 2795.362 | tokens/sec: 187556.356\n",
            "step: 20 | loss: 6.1219e+00 | lr: 5.2092e-04 | norm: 1.0160 | dt: 2793.905 | tokens/sec: 187654.196\n",
            "step: 21 | loss: 6.0652e+00 | lr: 5.0535e-04 | norm: 0.7396 | dt: 2796.125 | tokens/sec: 187505.228\n",
            "step: 22 | loss: 6.0528e+00 | lr: 4.8870e-04 | norm: 0.5129 | dt: 2797.505 | tokens/sec: 187412.687\n",
            "step: 23 | loss: 6.0058e+00 | lr: 4.7107e-04 | norm: 0.5364 | dt: 2799.362 | tokens/sec: 187288.361\n",
            "step: 24 | loss: 5.9921e+00 | lr: 4.5258e-04 | norm: 0.3759 | dt: 2798.284 | tokens/sec: 187360.568\n",
            "step: 25 | loss: 5.9803e+00 | lr: 4.3332e-04 | norm: 0.4035 | dt: 2797.323 | tokens/sec: 187424.923\n",
            "step: 26 | loss: 5.9581e+00 | lr: 4.1343e-04 | norm: 0.3779 | dt: 2799.333 | tokens/sec: 187290.323\n",
            "step: 27 | loss: 5.9715e+00 | lr: 3.9303e-04 | norm: 0.5028 | dt: 2801.677 | tokens/sec: 187133.604\n",
            "step: 28 | loss: 5.9409e+00 | lr: 3.7224e-04 | norm: 0.3425 | dt: 2800.979 | tokens/sec: 187180.259\n",
            "step: 29 | loss: 5.9348e+00 | lr: 3.5118e-04 | norm: 0.3138 | dt: 2802.432 | tokens/sec: 187083.199\n",
            "step: 30 | loss: 5.9285e+00 | lr: 3.3000e-04 | norm: 0.3036 | dt: 2801.517 | tokens/sec: 187144.29\n",
            "step: 31 | loss: 5.9067e+00 | lr: 3.0882e-04 | norm: 0.3131 | dt: 2801.547 | tokens/sec: 187142.331\n",
            "step: 32 | loss: 5.9129e+00 | lr: 2.8776e-04 | norm: 0.3298 | dt: 2802.056 | tokens/sec: 187108.334\n",
            "step: 33 | loss: 5.8817e+00 | lr: 2.6697e-04 | norm: 0.4356 | dt: 2800.876 | tokens/sec: 187187.142\n",
            "step: 34 | loss: 5.8731e+00 | lr: 2.4657e-04 | norm: 0.3442 | dt: 2804.218 | tokens/sec: 186964.079\n",
            "step: 35 | loss: 5.8729e+00 | lr: 2.2668e-04 | norm: 0.4603 | dt: 2802.439 | tokens/sec: 187082.722\n",
            "step: 36 | loss: 5.8528e+00 | lr: 2.0742e-04 | norm: 0.3293 | dt: 2805.028 | tokens/sec: 186910.064\n",
            "step: 37 | loss: 5.8694e+00 | lr: 1.8893e-04 | norm: 0.2964 | dt: 2802.788 | tokens/sec: 187059.439\n",
            "step: 38 | loss: 5.8398e+00 | lr: 1.7130e-04 | norm: 0.1898 | dt: 2805.127 | tokens/sec: 186903.471\n",
            "step: 39 | loss: 5.8372e+00 | lr: 1.5465e-04 | norm: 0.2502 | dt: 2805.473 | tokens/sec: 186880.456\n",
            "step: 40 | loss: 5.8377e+00 | lr: 1.3908e-04 | norm: 0.2116 | dt: 2806.83 | tokens/sec: 186790.085\n",
            "step: 41 | loss: 5.8210e+00 | lr: 1.2469e-04 | norm: 0.2774 | dt: 2806.5 | tokens/sec: 186812.015\n",
            "step: 42 | loss: 5.8393e+00 | lr: 1.1157e-04 | norm: 0.3213 | dt: 2808.754 | tokens/sec: 186662.146\n",
            "step: 43 | loss: 5.8121e+00 | lr: 9.9787e-05 | norm: 0.2529 | dt: 2807.379 | tokens/sec: 186753.583\n",
            "step: 44 | loss: 5.8100e+00 | lr: 8.9428e-05 | norm: 0.2053 | dt: 2806.783 | tokens/sec: 186793.179\n",
            "step: 45 | loss: 5.8107e+00 | lr: 8.0553e-05 | norm: 0.1902 | dt: 2810.667 | tokens/sec: 186535.096\n",
            "step: 46 | loss: 5.7958e+00 | lr: 7.3215e-05 | norm: 0.2263 | dt: 2810.092 | tokens/sec: 186573.269\n",
            "step: 47 | loss: 5.8153e+00 | lr: 6.7460e-05 | norm: 0.1800 | dt: 2810.418 | tokens/sec: 186551.632\n",
            "step: 48 | loss: 5.7883e+00 | lr: 6.3324e-05 | norm: 0.1719 | dt: 2810.855 | tokens/sec: 186522.612\n",
            "step: 49 | loss: 5.7878e+00 | lr: 6.0832e-05 | norm: 0.1622 | dt: 2808.4 | tokens/sec: 186685.631\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = GPT2(GPT2Config(vocab_size=50304)) # Initializing with random weights. Not using HF model.\n",
        "model.to(device)\n",
        "model = torch.compile(model)\n",
        "\n",
        "# Cosine decay learning rate with warm-up.\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "\n",
        "def get_lr(it):\n",
        "  # Linear warmp for warm_iter steps\n",
        "  if it < warmup_steps:\n",
        "    return max_lr * (it+1) / warmup_steps\n",
        "  if it > max_steps:\n",
        "    return min_lr\n",
        "  decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "  assert 0.0 <= decay_ratio <= 1.0\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "  return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, betas=(0.9, 0.95), device_type=device)\n",
        "\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(grand_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "          logits, loss = model(x, y)\n",
        "        loss = loss / grand_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.step()\n",
        "    torch.cuda.synchronize() # Wait for gpu to finish work.\n",
        "    t1 = time.time()\n",
        "    dt = round((t1 - t0)*1000, 3) # time difference in ms.\n",
        "    tokens_per_second = round((train_loader.B * train_loader.T * grand_accum_steps) / (t1-t0), 3)\n",
        "    print(f'step: {step} | loss: {loss_accum.item():.4e} | lr: {lr:.4e} | norm: {norm:.4f} | dt: {dt} | tokens/sec: {tokens_per_second}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47687437",
      "metadata": {
        "id": "47687437"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}