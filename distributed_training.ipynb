{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "57cf6a1d",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishvinodkumar/GPT2/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e57c99",
      "metadata": {
        "id": "64e57c99"
      },
      "source": [
        "# Distributed Training Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fbbcacd",
      "metadata": {
        "id": "0fbbcacd"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fb214508",
      "metadata": {
        "id": "fb214508"
      },
      "outputs": [],
      "source": [
        "from gpt2 import GPT2, GPT2Config, DataLoaderLite\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3483022",
      "metadata": {
        "id": "e3483022"
      },
      "source": [
        "### Set Device & Args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "85e5733b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85e5733b",
        "outputId": "8981a7b2-b9d5-4d78-9aec-4999ecb0fb93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "print(f'Using Device: {device}')\n",
        "\n",
        "num_return_sequences = 5\n",
        "max_length = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3c47d2dd",
      "metadata": {
        "id": "3c47d2dd"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "503b104c",
      "metadata": {
        "id": "503b104c"
      },
      "source": [
        "### Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "278d1209",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "278d1209",
        "outputId": "7780bc7f-5062-4b84-8de9-6bfe323f7dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Desired Batch Size: 524288\n",
            "Grand Accumulate Steps: 32\n",
            "Loaded 338025 tokens\n",
            "1 Epoch = 20 batches\n"
          ]
        }
      ],
      "source": [
        "total_batch_size = 524288 # 2**19, ~0.5M, as per GPT paper.\n",
        "B = 16 # Micro Batch Size\n",
        "T = 1024 # Max Sequence Length\n",
        "assert total_batch_size % (B*T) == 0, \"Total Batch Size must be divisible by B*T\"\n",
        "grand_accum_steps = total_batch_size // (B*T)\n",
        "\n",
        "print(f'Total Desired Batch Size: {total_batch_size}')\n",
        "print(f'Grand Accumulate Steps: {grand_accum_steps}')\n",
        "\n",
        "input_text = './data/input.txt'\n",
        "train_loader = DataLoaderLite(B=B, T=T, input_text=input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f9bd40",
      "metadata": {
        "id": "42f9bd40"
      },
      "source": [
        "### Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "XAOFOMOWLtYn",
      "metadata": {
        "id": "XAOFOMOWLtYn"
      },
      "outputs": [],
      "source": [
        "# Set precision to TF32 when available. Will speed up total performance.\n",
        "# TF32 will reduce the decimal precision.\n",
        "torch.set_float32_matmul_precision('high')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2e75a347",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e75a347",
        "outputId": "2bc2ef09-be87-4cba-fa1d-e09b06ed7732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step: 0 | loss: 1.0939e+01 | lr: 6.0000e-05 | norm: 27.0126 | dt: 14963.934 | tokens/sec: 35036.776\n",
            "step: 1 | loss: 9.6493e+00 | lr: 1.2000e-04 | norm: 9.5177 | dt: 2773.384 | tokens/sec: 189042.694\n",
            "step: 2 | loss: 9.2256e+00 | lr: 1.8000e-04 | norm: 5.7290 | dt: 2775.328 | tokens/sec: 188910.273\n",
            "step: 3 | loss: 9.8131e+00 | lr: 2.4000e-04 | norm: 8.2065 | dt: 2776.14 | tokens/sec: 188854.998\n",
            "step: 4 | loss: 9.1916e+00 | lr: 3.0000e-04 | norm: 4.2994 | dt: 2770.745 | tokens/sec: 189222.784\n",
            "step: 5 | loss: 8.6780e+00 | lr: 3.6000e-04 | norm: 3.6285 | dt: 2773.612 | tokens/sec: 189027.159\n",
            "step: 6 | loss: 8.2950e+00 | lr: 4.2000e-04 | norm: 1.9536 | dt: 2777.642 | tokens/sec: 188752.873\n",
            "step: 7 | loss: 8.0680e+00 | lr: 4.8000e-04 | norm: 2.8521 | dt: 2780.02 | tokens/sec: 188591.449\n",
            "step: 8 | loss: 7.7142e+00 | lr: 5.4000e-04 | norm: 1.9108 | dt: 2775.557 | tokens/sec: 188894.679\n",
            "step: 9 | loss: 7.3470e+00 | lr: 6.0000e-04 | norm: 1.8005 | dt: 2776.353 | tokens/sec: 188840.532\n",
            "step: 10 | loss: 7.0297e+00 | lr: 6.0000e-04 | norm: 1.8390 | dt: 2778.441 | tokens/sec: 188698.646\n",
            "step: 11 | loss: 6.7412e+00 | lr: 5.9917e-04 | norm: 1.5060 | dt: 2784.675 | tokens/sec: 188276.214\n",
            "step: 12 | loss: 6.5289e+00 | lr: 5.9668e-04 | norm: 1.1517 | dt: 2785.897 | tokens/sec: 188193.636\n",
            "step: 13 | loss: 6.3773e+00 | lr: 5.9254e-04 | norm: 1.0571 | dt: 2788.832 | tokens/sec: 187995.567\n",
            "step: 14 | loss: 6.3415e+00 | lr: 5.8679e-04 | norm: 2.6024 | dt: 2785.122 | tokens/sec: 188245.962\n",
            "step: 15 | loss: 6.2435e+00 | lr: 5.7945e-04 | norm: 0.9695 | dt: 2788.516 | tokens/sec: 188016.865\n",
            "step: 16 | loss: 6.2126e+00 | lr: 5.7057e-04 | norm: 0.7738 | dt: 2787.378 | tokens/sec: 188093.609\n",
            "step: 17 | loss: 6.2101e+00 | lr: 5.6021e-04 | norm: 1.1354 | dt: 2784.764 | tokens/sec: 188270.186\n",
            "step: 18 | loss: 6.1569e+00 | lr: 5.4843e-04 | norm: 1.0173 | dt: 2788.342 | tokens/sec: 188028.569\n",
            "step: 19 | loss: 6.1475e+00 | lr: 5.3531e-04 | norm: 1.5021 | dt: 2792.356 | tokens/sec: 187758.277\n",
            "step: 20 | loss: 6.1212e+00 | lr: 5.2092e-04 | norm: 0.9941 | dt: 2796.434 | tokens/sec: 187484.478\n",
            "step: 21 | loss: 6.0654e+00 | lr: 5.0535e-04 | norm: 0.7520 | dt: 2790.799 | tokens/sec: 187863.068\n",
            "step: 22 | loss: 6.0524e+00 | lr: 4.8870e-04 | norm: 0.5191 | dt: 2797.092 | tokens/sec: 187440.371\n",
            "step: 23 | loss: 6.0054e+00 | lr: 4.7107e-04 | norm: 0.5070 | dt: 2795.073 | tokens/sec: 187575.762\n",
            "step: 24 | loss: 5.9923e+00 | lr: 4.5258e-04 | norm: 0.3848 | dt: 2797.275 | tokens/sec: 187428.118\n",
            "step: 25 | loss: 5.9795e+00 | lr: 4.3332e-04 | norm: 0.3779 | dt: 2795.0 | tokens/sec: 187580.691\n",
            "step: 26 | loss: 5.9579e+00 | lr: 4.1343e-04 | norm: 0.3966 | dt: 2795.728 | tokens/sec: 187531.836\n",
            "step: 27 | loss: 5.9713e+00 | lr: 3.9303e-04 | norm: 0.4898 | dt: 2797.126 | tokens/sec: 187438.102\n",
            "step: 28 | loss: 5.9407e+00 | lr: 3.7224e-04 | norm: 0.3414 | dt: 2802.317 | tokens/sec: 187090.903\n",
            "step: 29 | loss: 5.9338e+00 | lr: 3.5118e-04 | norm: 0.3151 | dt: 2799.893 | tokens/sec: 187252.892\n",
            "step: 30 | loss: 5.9274e+00 | lr: 3.3000e-04 | norm: 0.3133 | dt: 2802.157 | tokens/sec: 187101.6\n",
            "step: 31 | loss: 5.9033e+00 | lr: 3.0882e-04 | norm: 0.3205 | dt: 2802.874 | tokens/sec: 187053.695\n",
            "step: 32 | loss: 5.9093e+00 | lr: 2.8776e-04 | norm: 0.3430 | dt: 2802.879 | tokens/sec: 187053.409\n",
            "step: 33 | loss: 5.8852e+00 | lr: 2.6697e-04 | norm: 0.5291 | dt: 2801.718 | tokens/sec: 187130.864\n",
            "step: 34 | loss: 5.8740e+00 | lr: 2.4657e-04 | norm: 0.3419 | dt: 2804.173 | tokens/sec: 186967.099\n",
            "step: 35 | loss: 5.8743e+00 | lr: 2.2668e-04 | norm: 0.5879 | dt: 2802.663 | tokens/sec: 187067.794\n",
            "step: 36 | loss: 5.8523e+00 | lr: 2.0742e-04 | norm: 0.2605 | dt: 2803.438 | tokens/sec: 187016.057\n",
            "step: 37 | loss: 5.8696e+00 | lr: 1.8893e-04 | norm: 0.3859 | dt: 2804.39 | tokens/sec: 186952.571\n",
            "step: 38 | loss: 5.8410e+00 | lr: 1.7130e-04 | norm: 0.2718 | dt: 2800.708 | tokens/sec: 187198.408\n",
            "step: 39 | loss: 5.8381e+00 | lr: 1.5465e-04 | norm: 0.2043 | dt: 2799.898 | tokens/sec: 187252.51\n",
            "step: 40 | loss: 5.8405e+00 | lr: 1.3908e-04 | norm: 0.2855 | dt: 2800.013 | tokens/sec: 187244.825\n",
            "step: 41 | loss: 5.8222e+00 | lr: 1.2469e-04 | norm: 0.2103 | dt: 2802.91 | tokens/sec: 187051.309\n",
            "step: 42 | loss: 5.8416e+00 | lr: 1.1157e-04 | norm: 0.2846 | dt: 2800.85 | tokens/sec: 187188.895\n",
            "step: 43 | loss: 5.8150e+00 | lr: 9.9787e-05 | norm: 0.2805 | dt: 2800.423 | tokens/sec: 187217.405\n",
            "step: 44 | loss: 5.8124e+00 | lr: 8.9428e-05 | norm: 0.1804 | dt: 2800.653 | tokens/sec: 187202.057\n",
            "step: 45 | loss: 5.8141e+00 | lr: 8.0553e-05 | norm: 0.2141 | dt: 2806.069 | tokens/sec: 186840.76\n",
            "step: 46 | loss: 5.7992e+00 | lr: 7.3215e-05 | norm: 0.2538 | dt: 2804.933 | tokens/sec: 186916.435\n",
            "step: 47 | loss: 5.8187e+00 | lr: 6.7460e-05 | norm: 0.1626 | dt: 2798.944 | tokens/sec: 187316.36\n",
            "step: 48 | loss: 5.7922e+00 | lr: 6.3324e-05 | norm: 0.1814 | dt: 2803.537 | tokens/sec: 187009.473\n",
            "step: 49 | loss: 5.7922e+00 | lr: 6.0832e-05 | norm: 0.2071 | dt: 2805.64 | tokens/sec: 186869.276\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = GPT2(GPT2Config(vocab_size=50304)) # Initializing with random weights. Not using HF model.\n",
        "model.to(device)\n",
        "model = torch.compile(model)\n",
        "\n",
        "# Cosine decay learning rate with warm-up.\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "\n",
        "def get_lr(it):\n",
        "  # Linear warmp for warm_iter steps\n",
        "  if it < warmup_steps:\n",
        "    return max_lr * (it+1) / warmup_steps\n",
        "  if it > max_steps:\n",
        "    return min_lr\n",
        "  decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "  assert 0.0 <= decay_ratio <= 1.0\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "  return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, betas=(0.9, 0.95), device_type=device)\n",
        "\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(grand_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "          logits, loss = model(x, y)\n",
        "        loss = loss / grand_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.step()\n",
        "    torch.cuda.synchronize() # Wait for gpu to finish work.\n",
        "    t1 = time.time()\n",
        "    dt = round((t1 - t0)*1000, 3) # time difference in ms.\n",
        "    tokens_per_second = round((train_loader.B * train_loader.T * grand_accum_steps) / (t1-t0), 3)\n",
        "    print(f'step: {step} | loss: {loss_accum.item():.4e} | lr: {lr:.4e} | norm: {norm:.4f} | dt: {dt} | tokens/sec: {tokens_per_second}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47687437",
      "metadata": {
        "id": "47687437"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
